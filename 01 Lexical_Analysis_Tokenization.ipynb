{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240b68c2-cf1f-4231-acc2-80973b4d3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python`\n",
    "#-*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3de084d0-bac9-4ad5-8b5b-36d2f276119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk spacy textblob -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776710e3-cd03-47e7-b22d-756be9bf33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, MWETokenizer, WhitespaceTokenizer\n",
    "from nltk.tokenize import SpaceTokenizer, TabTokenizer, LineTokenizer, TweetTokenizer\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2efe1e6-e248-4e03-9605-3dc339bed998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package indian to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') #tokenization\n",
    "nltk.download('stopwords') #stopwords removal\n",
    "nltk.download('averaged_perceptron_tagger') #POS tagging\n",
    "nltk.download('wordnet') #wordnet database and lemmatization\n",
    "nltk.download('omw-1.4') #stemming\n",
    "nltk.download('indian') #Indian language POS tagging\n",
    "nltk.download('maxent_ne_chunker') #chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c1419-aad2-4f35-af71-ba08bc0b853d",
   "metadata": {},
   "source": [
    "### sample example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8775cb-5d62-4c84-a66f-bba94f50b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'They told that their ages are 25 27 and 31 respectively.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c505bd-4a3d-46cf-81a4-f820184f7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the average of ages mentioned in the sentence above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ba8d6f-5914-4697-b935-6353464f9fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class str in module builtins:\n",
      "\n",
      "class str(object)\n",
      " |  str(object='') -> str\n",
      " |  str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
      " |  \n",
      " |  Create a new string object from the given object. If encoding or\n",
      " |  errors is specified, then the object must expose a data buffer\n",
      " |  that will be decoded using the given encoding and error handler.\n",
      " |  Otherwise, returns the result of object.__str__() (if defined)\n",
      " |  or repr(object).\n",
      " |  encoding defaults to sys.getdefaultencoding().\n",
      " |  errors defaults to 'strict'.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __format__(self, format_spec, /)\n",
      " |      Return a formatted version of the string as described by format_spec.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mod__(self, value, /)\n",
      " |      Return self%value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__(self, value, /)\n",
      " |      Return value%self.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  __sizeof__(self, /)\n",
      " |      Return the size of the string in memory, in bytes.\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  capitalize(self, /)\n",
      " |      Return a capitalized version of the string.\n",
      " |      \n",
      " |      More specifically, make the first character have upper case and the rest lower\n",
      " |      case.\n",
      " |  \n",
      " |  casefold(self, /)\n",
      " |      Return a version of the string suitable for caseless comparisons.\n",
      " |  \n",
      " |  center(self, width, fillchar=' ', /)\n",
      " |      Return a centered string of length width.\n",
      " |      \n",
      " |      Padding is done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  count(...)\n",
      " |      S.count(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the number of non-overlapping occurrences of substring sub in\n",
      " |      string S[start:end].  Optional arguments start and end are\n",
      " |      interpreted as in slice notation.\n",
      " |  \n",
      " |  encode(self, /, encoding='utf-8', errors='strict')\n",
      " |      Encode the string using the codec registered for encoding.\n",
      " |      \n",
      " |      encoding\n",
      " |        The encoding in which to encode the string.\n",
      " |      errors\n",
      " |        The error handling scheme to use for encoding errors.\n",
      " |        The default is 'strict' meaning that encoding errors raise a\n",
      " |        UnicodeEncodeError.  Other possible values are 'ignore', 'replace' and\n",
      " |        'xmlcharrefreplace' as well as any other name registered with\n",
      " |        codecs.register_error that can handle UnicodeEncodeErrors.\n",
      " |  \n",
      " |  endswith(...)\n",
      " |      S.endswith(suffix[, start[, end]]) -> bool\n",
      " |      \n",
      " |      Return True if S ends with the specified suffix, False otherwise.\n",
      " |      With optional start, test S beginning at that position.\n",
      " |      With optional end, stop comparing S at that position.\n",
      " |      suffix can also be a tuple of strings to try.\n",
      " |  \n",
      " |  expandtabs(self, /, tabsize=8)\n",
      " |      Return a copy where all tab characters are expanded using spaces.\n",
      " |      \n",
      " |      If tabsize is not given, a tab size of 8 characters is assumed.\n",
      " |  \n",
      " |  find(...)\n",
      " |      S.find(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the lowest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Return -1 on failure.\n",
      " |  \n",
      " |  format(...)\n",
      " |      S.format(*args, **kwargs) -> str\n",
      " |      \n",
      " |      Return a formatted version of S, using substitutions from args and kwargs.\n",
      " |      The substitutions are identified by braces ('{' and '}').\n",
      " |  \n",
      " |  format_map(...)\n",
      " |      S.format_map(mapping) -> str\n",
      " |      \n",
      " |      Return a formatted version of S, using substitutions from mapping.\n",
      " |      The substitutions are identified by braces ('{' and '}').\n",
      " |  \n",
      " |  index(...)\n",
      " |      S.index(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the lowest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Raises ValueError when the substring is not found.\n",
      " |  \n",
      " |  isalnum(self, /)\n",
      " |      Return True if the string is an alpha-numeric string, False otherwise.\n",
      " |      \n",
      " |      A string is alpha-numeric if all characters in the string are alpha-numeric and\n",
      " |      there is at least one character in the string.\n",
      " |  \n",
      " |  isalpha(self, /)\n",
      " |      Return True if the string is an alphabetic string, False otherwise.\n",
      " |      \n",
      " |      A string is alphabetic if all characters in the string are alphabetic and there\n",
      " |      is at least one character in the string.\n",
      " |  \n",
      " |  isascii(self, /)\n",
      " |      Return True if all characters in the string are ASCII, False otherwise.\n",
      " |      \n",
      " |      ASCII characters have code points in the range U+0000-U+007F.\n",
      " |      Empty string is ASCII too.\n",
      " |  \n",
      " |  isdecimal(self, /)\n",
      " |      Return True if the string is a decimal string, False otherwise.\n",
      " |      \n",
      " |      A string is a decimal string if all characters in the string are decimal and\n",
      " |      there is at least one character in the string.\n",
      " |  \n",
      " |  isdigit(self, /)\n",
      " |      Return True if the string is a digit string, False otherwise.\n",
      " |      \n",
      " |      A string is a digit string if all characters in the string are digits and there\n",
      " |      is at least one character in the string.\n",
      " |  \n",
      " |  isidentifier(self, /)\n",
      " |      Return True if the string is a valid Python identifier, False otherwise.\n",
      " |      \n",
      " |      Call keyword.iskeyword(s) to test whether string s is a reserved identifier,\n",
      " |      such as \"def\" or \"class\".\n",
      " |  \n",
      " |  islower(self, /)\n",
      " |      Return True if the string is a lowercase string, False otherwise.\n",
      " |      \n",
      " |      A string is lowercase if all cased characters in the string are lowercase and\n",
      " |      there is at least one cased character in the string.\n",
      " |  \n",
      " |  isnumeric(self, /)\n",
      " |      Return True if the string is a numeric string, False otherwise.\n",
      " |      \n",
      " |      A string is numeric if all characters in the string are numeric and there is at\n",
      " |      least one character in the string.\n",
      " |  \n",
      " |  isprintable(self, /)\n",
      " |      Return True if the string is printable, False otherwise.\n",
      " |      \n",
      " |      A string is printable if all of its characters are considered printable in\n",
      " |      repr() or if it is empty.\n",
      " |  \n",
      " |  isspace(self, /)\n",
      " |      Return True if the string is a whitespace string, False otherwise.\n",
      " |      \n",
      " |      A string is whitespace if all characters in the string are whitespace and there\n",
      " |      is at least one character in the string.\n",
      " |  \n",
      " |  istitle(self, /)\n",
      " |      Return True if the string is a title-cased string, False otherwise.\n",
      " |      \n",
      " |      In a title-cased string, upper- and title-case characters may only\n",
      " |      follow uncased characters and lowercase characters only cased ones.\n",
      " |  \n",
      " |  isupper(self, /)\n",
      " |      Return True if the string is an uppercase string, False otherwise.\n",
      " |      \n",
      " |      A string is uppercase if all cased characters in the string are uppercase and\n",
      " |      there is at least one cased character in the string.\n",
      " |  \n",
      " |  join(self, iterable, /)\n",
      " |      Concatenate any number of strings.\n",
      " |      \n",
      " |      The string whose method is called is inserted in between each given string.\n",
      " |      The result is returned as a new string.\n",
      " |      \n",
      " |      Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'\n",
      " |  \n",
      " |  ljust(self, width, fillchar=' ', /)\n",
      " |      Return a left-justified string of length width.\n",
      " |      \n",
      " |      Padding is done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  lower(self, /)\n",
      " |      Return a copy of the string converted to lowercase.\n",
      " |  \n",
      " |  lstrip(self, chars=None, /)\n",
      " |      Return a copy of the string with leading whitespace removed.\n",
      " |      \n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  partition(self, sep, /)\n",
      " |      Partition the string into three parts using the given separator.\n",
      " |      \n",
      " |      This will search for the separator in the string.  If the separator is found,\n",
      " |      returns a 3-tuple containing the part before the separator, the separator\n",
      " |      itself, and the part after it.\n",
      " |      \n",
      " |      If the separator is not found, returns a 3-tuple containing the original string\n",
      " |      and two empty strings.\n",
      " |  \n",
      " |  removeprefix(self, prefix, /)\n",
      " |      Return a str with the given prefix string removed if present.\n",
      " |      \n",
      " |      If the string starts with the prefix string, return string[len(prefix):].\n",
      " |      Otherwise, return a copy of the original string.\n",
      " |  \n",
      " |  removesuffix(self, suffix, /)\n",
      " |      Return a str with the given suffix string removed if present.\n",
      " |      \n",
      " |      If the string ends with the suffix string and that suffix is not empty,\n",
      " |      return string[:-len(suffix)]. Otherwise, return a copy of the original\n",
      " |      string.\n",
      " |  \n",
      " |  replace(self, old, new, count=-1, /)\n",
      " |      Return a copy with all occurrences of substring old replaced by new.\n",
      " |      \n",
      " |        count\n",
      " |          Maximum number of occurrences to replace.\n",
      " |          -1 (the default value) means replace all occurrences.\n",
      " |      \n",
      " |      If the optional argument count is given, only the first count occurrences are\n",
      " |      replaced.\n",
      " |  \n",
      " |  rfind(...)\n",
      " |      S.rfind(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the highest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Return -1 on failure.\n",
      " |  \n",
      " |  rindex(...)\n",
      " |      S.rindex(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the highest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Raises ValueError when the substring is not found.\n",
      " |  \n",
      " |  rjust(self, width, fillchar=' ', /)\n",
      " |      Return a right-justified string of length width.\n",
      " |      \n",
      " |      Padding is done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  rpartition(self, sep, /)\n",
      " |      Partition the string into three parts using the given separator.\n",
      " |      \n",
      " |      This will search for the separator in the string, starting at the end. If\n",
      " |      the separator is found, returns a 3-tuple containing the part before the\n",
      " |      separator, the separator itself, and the part after it.\n",
      " |      \n",
      " |      If the separator is not found, returns a 3-tuple containing two empty strings\n",
      " |      and the original string.\n",
      " |  \n",
      " |  rsplit(self, /, sep=None, maxsplit=-1)\n",
      " |      Return a list of the substrings in the string, using sep as the separator string.\n",
      " |      \n",
      " |        sep\n",
      " |          The separator used to split the string.\n",
      " |      \n",
      " |          When set to None (the default value), will split on any whitespace\n",
      " |          character (including \\n \\r \\t \\f and spaces) and will discard\n",
      " |          empty strings from the result.\n",
      " |        maxsplit\n",
      " |          Maximum number of splits (starting from the left).\n",
      " |          -1 (the default value) means no limit.\n",
      " |      \n",
      " |      Splitting starts at the end of the string and works to the front.\n",
      " |  \n",
      " |  rstrip(self, chars=None, /)\n",
      " |      Return a copy of the string with trailing whitespace removed.\n",
      " |      \n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  split(self, /, sep=None, maxsplit=-1)\n",
      " |      Return a list of the substrings in the string, using sep as the separator string.\n",
      " |      \n",
      " |        sep\n",
      " |          The separator used to split the string.\n",
      " |      \n",
      " |          When set to None (the default value), will split on any whitespace\n",
      " |          character (including \\n \\r \\t \\f and spaces) and will discard\n",
      " |          empty strings from the result.\n",
      " |        maxsplit\n",
      " |          Maximum number of splits (starting from the left).\n",
      " |          -1 (the default value) means no limit.\n",
      " |      \n",
      " |      Note, str.split() is mainly useful for data that has been intentionally\n",
      " |      delimited.  With natural text that includes punctuation, consider using\n",
      " |      the regular expression module.\n",
      " |  \n",
      " |  splitlines(self, /, keepends=False)\n",
      " |      Return a list of the lines in the string, breaking at line boundaries.\n",
      " |      \n",
      " |      Line breaks are not included in the resulting list unless keepends is given and\n",
      " |      true.\n",
      " |  \n",
      " |  startswith(...)\n",
      " |      S.startswith(prefix[, start[, end]]) -> bool\n",
      " |      \n",
      " |      Return True if S starts with the specified prefix, False otherwise.\n",
      " |      With optional start, test S beginning at that position.\n",
      " |      With optional end, stop comparing S at that position.\n",
      " |      prefix can also be a tuple of strings to try.\n",
      " |  \n",
      " |  strip(self, chars=None, /)\n",
      " |      Return a copy of the string with leading and trailing whitespace removed.\n",
      " |      \n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  swapcase(self, /)\n",
      " |      Convert uppercase characters to lowercase and lowercase characters to uppercase.\n",
      " |  \n",
      " |  title(self, /)\n",
      " |      Return a version of the string where each word is titlecased.\n",
      " |      \n",
      " |      More specifically, words start with uppercased characters and all remaining\n",
      " |      cased characters have lower case.\n",
      " |  \n",
      " |  translate(self, table, /)\n",
      " |      Replace each character in the string using the given translation table.\n",
      " |      \n",
      " |        table\n",
      " |          Translation table, which must be a mapping of Unicode ordinals to\n",
      " |          Unicode ordinals, strings, or None.\n",
      " |      \n",
      " |      The table must implement lookup/indexing via __getitem__, for instance a\n",
      " |      dictionary or list.  If this operation raises LookupError, the character is\n",
      " |      left untouched.  Characters mapped to None are deleted.\n",
      " |  \n",
      " |  upper(self, /)\n",
      " |      Return a copy of the string converted to uppercase.\n",
      " |  \n",
      " |  zfill(self, width, /)\n",
      " |      Pad a numeric string with zeros on the left, to fill a field of the given width.\n",
      " |      \n",
      " |      The string is never truncated.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  maketrans(...)\n",
      " |      Return a translation table usable for str.translate().\n",
      " |      \n",
      " |      If there is only one argument, it must be a dictionary mapping Unicode\n",
      " |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
      " |      Character keys will be then converted to ordinals.\n",
      " |      If there are two arguments, they must be strings of equal length, and\n",
      " |      in the resulting dictionary, each character in x will be mapped to the\n",
      " |      character at the same position in y. If there is a third argument, it\n",
      " |      must be a string, whose characters will be mapped to None in the result.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26a39e9-6437-4cbe-9d00-b78655ceb825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'told',\n",
       " 'that',\n",
       " 'their',\n",
       " 'ages',\n",
       " 'are',\n",
       " '25',\n",
       " '27',\n",
       " 'and',\n",
       " '31',\n",
       " 'respectively.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "smpl_lst = sent.split(' ')\n",
    "smpl_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "577da765-c12c-4c28-997e-dcdaf1a79209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lst = []\n",
    "sum = 0\n",
    "for i in smpl_lst :\n",
    "    if i.isdigit() :\n",
    "        num = int(i)\n",
    "        num_lst.append(num)\n",
    "        sum += num\n",
    "        \n",
    "num_lst\n",
    "avg = sum / len(num_lst)\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61350e36-5572-4902-9de5-8db4f1b6856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = []\n",
    "for word in sent.split() :\n",
    "    if word.isdigit() :\n",
    "        ages.append(int(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24add41f-d0d4-4584-a6df-fa172eb0ed0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ages) / len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be9382fb-f230-480c-bb64-90a5f1d16b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages = [int(word) for word in sent.split() if word.isdigit()]\n",
    "avg = np.sum(ages) / len(ages)\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08872e93-3309-45d9-8634-a1d550737e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([int(word) for word in sent.split() if word.isdigit()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df16b8-8a44-4b9d-85b4-7dfd75089565",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad1bb4ca-576b-4242-8ea2-7ebe6bac3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello friends! How are you? Welcome to Python Programming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "207b6eb2-0961-4fdc-8147-f0909aa634df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#segmentation\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e6e9f38-7cd1-4af2-9f84-d840166d5ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst1 = word_tokenize(sent)\n",
    "lst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d206a1c-9f30-4762-87d7-e9d08f578dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find percentage of punctuation symbols present in it\n",
    "# method 1\n",
    "countpun = 0\n",
    "for i in lst1 :\n",
    "    if  i.isalnum() == False:\n",
    "        countpun+=1\n",
    "per =  (countpun / len(lst1)) * 100\n",
    "per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4da0c96d-5ef3-48cb-b517-a7b111eb3e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_count = len([word for word in word_tokenize(sent) if not word.isalnum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3deb0eb0-cb43-4545-8f5d-31249c120ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_count / len(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40dac2dd-1a3e-45db-89fa-b23e75e90650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a6fc063-d0e4-447a-98fe-2e4807867ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('#')  #gives ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb9aa660-2133-4147-88c8-e0cda41192f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gives how much memory required in bytes\n",
    "sys.getsizeof('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92315bf0-26a2-4180-9a35-4b21484ab9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function getsizeof in module sys:\n",
      "\n",
      "getsizeof(...)\n",
      "    getsizeof(object [, default]) -> int\n",
      "    \n",
      "    Return the size of object in bytes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sys.getsizeof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b980fcba-4b95-48ec-880b-4e0c880c5fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = 'abcdge'\n",
    "sys.getsizeof(char)  # here minimum of 50 is assigned and increases with added characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4614dede-5992-494d-8ccb-7bbecdeec438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(67) #from ascii to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3169a0f-4e8e-43c7-8626-57f2be1bd8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§µ‡•Ä\n",
      "‡§∏\n"
     ]
    }
   ],
   "source": [
    "char = '\\u0935\\u0940'  #unicode for devnagari\n",
    "print(char)\n",
    "char1 = '\\u0938'\n",
    "print(char1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38ebc493-c931-4385-86c5-139314f7e677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2357"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('‡§µ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3fb406d-7167-4265-8746-850124200a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§∂'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(2358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "937c5288-2790-45f5-8c47-103659a0f00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§µ'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0x935)   #hexadecimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f3ad023-94b0-4a0c-b1ed-ac03eb55deee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('‡§µ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fde26833-3b51-4560-a9cf-f4a741b64e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '‡§Æ‡§ø‡§§‡§æ‡§≤‡•Ä ‡§ó‡§ø‡§∞‡•Ä'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9074fc14-792b-44f7-bc79-cf58d2e2a789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§Æ‡§ø‡§§‡§æ‡§≤‡•Ä', '‡§ó‡§ø‡§∞‡•Ä']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4144ff3-aac2-4dcc-b51a-73d508c7ad9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.startswith('‡§Æ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d52dff6-cca6-41a2-8337-ca5c86265227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Æ‡•Ä‡§§‡§æ‡§≤‡•Ä ‡§ó‡§ø‡§∞‡•Ä'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.replace('‡§Æ‡§ø', '‡§Æ‡•Ä')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c9187aa-b51f-48c8-b8f6-c1d10bc20583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.find('‡§ó')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47e1a97b-e8a6-4f9c-8fa6-fc3b3730ce8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39ae17dd-9667-4192-872c-f0e81b15ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [ '‡§Æ‡§ø‡§§‡§æ‡§≤‡•Ä', '‡§Æ‡•É‡§£‡§æ‡§≤', '‡§Ö‡§Ç‡§ó‡§ø‡§∞‡§æ', '‡§§‡•Å‡§≤‡§∏‡•Ä', '‡§™‡•ç‡§∞‡§ö‡•á‡§§', '‡§Ö‡§Æ‡•á‡§Ø', '‡§™‡•ç‡§∞‡§£‡§µ' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ce9ada6-2e23-46db-9816-713a4a8e782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§™‡•ç‡§∞‡§ö‡•á‡§§\n",
      "‡§™‡•ç‡§∞‡§£‡§µ\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    if name.startswith('‡§™‡•ç‡§∞'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2eee6a6-9acd-46f2-bc47-082877e46b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtext = '‡§á‡§§‡§ø‡§π‡§æ‡§∏ - ‡§∞‡§æ‡§ú‡§ó‡§° ‡§ï‡§ø‡§≤‡•ç‡§≤‡§æ ‡§á‡§∏‡§µ‡•Ä ‡§∏‡§®‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§™‡§π‡§ø‡§≤‡•ç‡§Ø‡§æ ‡§∂‡§§‡§ï‡§æ‡§§‡§≤‡§æ ‡§Ü‡§π‡•á. ‡§Ø‡§æ ‡§Æ‡•Å‡§∞‡•Ç‡§Ç‡§¨‡§¶‡•á‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§°‡•ã‡§Ç‡§ó‡§∞‡§æ‡§≤‡§æ ‡§ï‡§ø‡§≤‡•ç‡§≤‡•ç‡§Ø‡§æ‡§ö‡•á ‡§∏‡•ç‡§µ‡§∞‡•Ç‡§™ ‡§ó‡•å‡§§‡§Æ‡•Ä‡§™‡•Å‡§§‡•ç‡§∞ ‡§∏‡§æ‡§§‡§ï‡§∞‡•ç‡§£‡•Ä ‡§Æ‡•ç‡§π‡§£‡§ú‡•á‡§ö ‡§∂‡§æ‡§≤‡§ø‡§µ‡§æ‡§π‡§® ‡§∞‡§æ‡§ú‡§æ ‡§ú‡•ç‡§Ø‡§æ‡§®‡•á ‡§Ø‡•Å‡§¶‡•ç‡§ß‡§æ‡§§ ‡§∂‡§ï‡§æ‡§Ç‡§®‡§æ ‡§π‡§æ‡§∞‡§µ‡•Ç‡§® ‡§á.‡§∏ ‡•≠‡•Æ ‡§∏‡§æ‡§≤‡•Ä ‡§∏‡•ç‡§µ‡§§‡§É‡§ö‡•ç‡§Ø‡§æ ‡§®‡§æ‡§µ‡§æ‡§ö‡§Ç ‡§∂‡§ï ‡§∏‡•Å‡§∞‡•Ç ‡§ï‡•á‡§≤‡•á, ‡§Ø‡§æ‡§®‡•á‡§ö ‡§Ü‡§† ‡§µ‡§∞‡•ç‡§∑‡§æ‡§Ç‡§™‡•Ç‡§∞‡•ç‡§µ‡•Ä ‡§á.‡§∏ ‡•≠‡•¶ ‡§∏‡§æ‡§≤‡•Ä ‡§π‡•ç‡§Ø‡§æ ‡§Æ‡•Å‡§∞‡•Ç‡§Ç‡§¨‡§¶‡•á‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§°‡•ã‡§Ç‡§ó‡§∞‡§æ‡§µ‡§∞ ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§Ö‡§∏‡§æ ‡§µ ‡§â‡§Ç‡§ö‡§™‡•Å‡§∞‡§æ ‡§∞‡§æ‡§ú‡§ó‡§° ‡§ï‡§ø‡§≤‡•ç‡§≤‡§æ ‡§¨‡§æ‡§Ç‡§ß‡§≤‡§æ. ‡§¨‡§π‡•Å‡§ß‡§æ ‡§∏‡§® ‡•ß‡•¨‡•™‡•´ ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§∂‡§ø‡§µ‡§æ‡§ú‡•Ä ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú‡§æ‡§Ç‡§®‡•Ä ‡§π‡§æ ‡§ï‡§ø‡§≤‡•ç‡§≤‡§æ ‡§§‡§æ‡§¨‡•ç‡§Ø‡§æ‡§§ ‡§ò‡•á‡§ä‡§® ‡§§‡•ç‡§Ø‡§æ‡§µ‡§∞ ‡§¨‡§æ‡§Ç‡§ß‡§ï‡§æ‡§Æ ‡§ï‡•á‡§≤‡•á ‡§µ ‡§§‡•ç‡§Ø‡§æ‡§ö‡•á ‡§®‡§æ‡§µ ‡§∞‡§æ‡§ú‡§ó‡§° ‡§†‡•á‡§µ‡§≤‡•á. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69c484b8-9762-4009-9424-812122bfc3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§á‡§§‡§ø‡§π‡§æ‡§∏',\n",
       " '-',\n",
       " '‡§∞‡§æ‡§ú‡§ó‡§°',\n",
       " '‡§ï‡§ø‡§≤‡•ç‡§≤‡§æ',\n",
       " '‡§á‡§∏‡§µ‡•Ä',\n",
       " '‡§∏‡§®‡§æ‡§ö‡•ç‡§Ø‡§æ',\n",
       " '‡§™‡§π‡§ø‡§≤‡•ç‡§Ø‡§æ',\n",
       " '‡§∂‡§§‡§ï‡§æ‡§§‡§≤‡§æ',\n",
       " '‡§Ü‡§π‡•á',\n",
       " '.',\n",
       " '‡§Ø‡§æ',\n",
       " '‡§Æ‡•Å‡§∞‡•Ç‡§Ç‡§¨‡§¶‡•á‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ',\n",
       " '‡§°‡•ã‡§Ç‡§ó‡§∞‡§æ‡§≤‡§æ',\n",
       " '‡§ï‡§ø‡§≤‡•ç‡§≤‡•ç‡§Ø‡§æ‡§ö‡•á',\n",
       " '‡§∏‡•ç‡§µ‡§∞‡•Ç‡§™',\n",
       " '‡§ó‡•å‡§§‡§Æ‡•Ä‡§™‡•Å‡§§‡•ç‡§∞',\n",
       " '‡§∏‡§æ‡§§‡§ï‡§∞‡•ç‡§£‡•Ä',\n",
       " '‡§Æ‡•ç‡§π‡§£‡§ú‡•á‡§ö',\n",
       " '‡§∂‡§æ‡§≤‡§ø‡§µ‡§æ‡§π‡§®',\n",
       " '‡§∞‡§æ‡§ú‡§æ',\n",
       " '‡§ú‡•ç‡§Ø‡§æ‡§®‡•á',\n",
       " '‡§Ø‡•Å‡§¶‡•ç‡§ß‡§æ‡§§',\n",
       " '‡§∂‡§ï‡§æ‡§Ç‡§®‡§æ',\n",
       " '‡§π‡§æ‡§∞‡§µ‡•Ç‡§®',\n",
       " '‡§á.‡§∏',\n",
       " '‡•≠‡•Æ',\n",
       " '‡§∏‡§æ‡§≤‡•Ä',\n",
       " '‡§∏‡•ç‡§µ‡§§‡§É‡§ö‡•ç‡§Ø‡§æ',\n",
       " '‡§®‡§æ‡§µ‡§æ‡§ö‡§Ç',\n",
       " '‡§∂‡§ï',\n",
       " '‡§∏‡•Å‡§∞‡•Ç',\n",
       " '‡§ï‡•á‡§≤‡•á',\n",
       " ',',\n",
       " '‡§Ø‡§æ‡§®‡•á‡§ö',\n",
       " '‡§Ü‡§†',\n",
       " '‡§µ‡§∞‡•ç‡§∑‡§æ‡§Ç‡§™‡•Ç‡§∞‡•ç‡§µ‡•Ä',\n",
       " '‡§á.‡§∏',\n",
       " '‡•≠‡•¶',\n",
       " '‡§∏‡§æ‡§≤‡•Ä',\n",
       " '‡§π‡•ç‡§Ø‡§æ',\n",
       " '‡§Æ‡•Å‡§∞‡•Ç‡§Ç‡§¨‡§¶‡•á‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ',\n",
       " '‡§°‡•ã‡§Ç‡§ó‡§∞‡§æ‡§µ‡§∞',\n",
       " '‡§∏‡•Å‡§Ç‡§¶‡§∞',\n",
       " '‡§Ö‡§∏‡§æ',\n",
       " '‡§µ',\n",
       " '‡§â‡§Ç‡§ö‡§™‡•Å‡§∞‡§æ',\n",
       " '‡§∞‡§æ‡§ú‡§ó‡§°',\n",
       " '‡§ï‡§ø‡§≤‡•ç‡§≤‡§æ',\n",
       " '‡§¨‡§æ‡§Ç‡§ß‡§≤‡§æ',\n",
       " '.',\n",
       " '‡§¨‡§π‡•Å‡§ß‡§æ',\n",
       " '‡§∏‡§®',\n",
       " '‡•ß‡•¨‡•™‡•´',\n",
       " '‡§Æ‡§ß‡•ç‡§Ø‡•á',\n",
       " '‡§∂‡§ø‡§µ‡§æ‡§ú‡•Ä',\n",
       " '‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú‡§æ‡§Ç‡§®‡•Ä',\n",
       " '‡§π‡§æ',\n",
       " '‡§ï‡§ø‡§≤‡•ç‡§≤‡§æ',\n",
       " '‡§§‡§æ‡§¨‡•ç‡§Ø‡§æ‡§§',\n",
       " '‡§ò‡•á‡§ä‡§®',\n",
       " '‡§§‡•ç‡§Ø‡§æ‡§µ‡§∞',\n",
       " '‡§¨‡§æ‡§Ç‡§ß‡§ï‡§æ‡§Æ',\n",
       " '‡§ï‡•á‡§≤‡•á',\n",
       " '‡§µ',\n",
       " '‡§§‡•ç‡§Ø‡§æ‡§ö‡•á',\n",
       " '‡§®‡§æ‡§µ',\n",
       " '‡§∞‡§æ‡§ú‡§ó‡§°',\n",
       " '‡§†‡•á‡§µ‡§≤‡•á',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(mtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50ef09ab-ad16-4f46-887e-ae29b75d76b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§á‡§§‡§ø‡§π‡§æ‡§∏ - ‡§∞‡§æ‡§ú‡§ó‡§° ‡§ï‡§ø‡§≤‡•ç‡§≤‡§æ ‡§á‡§∏‡§µ‡•Ä ‡§∏‡§®‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§™‡§π‡§ø‡§≤‡•ç‡§Ø‡§æ ‡§∂‡§§‡§ï‡§æ‡§§‡§≤‡§æ ‡§Ü‡§π‡•á.',\n",
       " '‡§Ø‡§æ ‡§Æ‡•Å‡§∞‡•Ç‡§Ç‡§¨‡§¶‡•á‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§°‡•ã‡§Ç‡§ó‡§∞‡§æ‡§≤‡§æ ‡§ï‡§ø‡§≤‡•ç‡§≤‡•ç‡§Ø‡§æ‡§ö‡•á ‡§∏‡•ç‡§µ‡§∞‡•Ç‡§™ ‡§ó‡•å‡§§‡§Æ‡•Ä‡§™‡•Å‡§§‡•ç‡§∞ ‡§∏‡§æ‡§§‡§ï‡§∞‡•ç‡§£‡•Ä ‡§Æ‡•ç‡§π‡§£‡§ú‡•á‡§ö ‡§∂‡§æ‡§≤‡§ø‡§µ‡§æ‡§π‡§® ‡§∞‡§æ‡§ú‡§æ ‡§ú‡•ç‡§Ø‡§æ‡§®‡•á ‡§Ø‡•Å‡§¶‡•ç‡§ß‡§æ‡§§ ‡§∂‡§ï‡§æ‡§Ç‡§®‡§æ ‡§π‡§æ‡§∞‡§µ‡•Ç‡§® ‡§á.‡§∏ ‡•≠‡•Æ ‡§∏‡§æ‡§≤‡•Ä ‡§∏‡•ç‡§µ‡§§‡§É‡§ö‡•ç‡§Ø‡§æ ‡§®‡§æ‡§µ‡§æ‡§ö‡§Ç ‡§∂‡§ï ‡§∏‡•Å‡§∞‡•Ç ‡§ï‡•á‡§≤‡•á, ‡§Ø‡§æ‡§®‡•á‡§ö ‡§Ü‡§† ‡§µ‡§∞‡•ç‡§∑‡§æ‡§Ç‡§™‡•Ç‡§∞‡•ç‡§µ‡•Ä ‡§á.‡§∏ ‡•≠‡•¶ ‡§∏‡§æ‡§≤‡•Ä ‡§π‡•ç‡§Ø‡§æ ‡§Æ‡•Å‡§∞‡•Ç‡§Ç‡§¨‡§¶‡•á‡§µ‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§°‡•ã‡§Ç‡§ó‡§∞‡§æ‡§µ‡§∞ ‡§∏‡•Å‡§Ç‡§¶‡§∞ ‡§Ö‡§∏‡§æ ‡§µ ‡§â‡§Ç‡§ö‡§™‡•Å‡§∞‡§æ ‡§∞‡§æ‡§ú‡§ó‡§° ‡§ï‡§ø‡§≤‡•ç‡§≤‡§æ ‡§¨‡§æ‡§Ç‡§ß‡§≤‡§æ.',\n",
       " '‡§¨‡§π‡•Å‡§ß‡§æ ‡§∏‡§® ‡•ß‡•¨‡•™‡•´ ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§∂‡§ø‡§µ‡§æ‡§ú‡•Ä ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú‡§æ‡§Ç‡§®‡•Ä ‡§π‡§æ ‡§ï‡§ø‡§≤‡•ç‡§≤‡§æ ‡§§‡§æ‡§¨‡•ç‡§Ø‡§æ‡§§ ‡§ò‡•á‡§ä‡§® ‡§§‡•ç‡§Ø‡§æ‡§µ‡§∞ ‡§¨‡§æ‡§Ç‡§ß‡§ï‡§æ‡§Æ ‡§ï‡•á‡§≤‡•á ‡§µ ‡§§‡•ç‡§Ø‡§æ‡§ö‡•á ‡§®‡§æ‡§µ ‡§∞‡§æ‡§ú‡§ó‡§° ‡§†‡•á‡§µ‡§≤‡•á.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(mtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaaf1d2-f8eb-4c88-9ab0-18be6b59e113",
   "metadata": {},
   "source": [
    "## 1. Space tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91be9199-db87-49b4-aad7-fd82d370dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('mydata.txt', encoding='utf-8')\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ce81d19-bd8f-4acf-a8dd-5a3119d728fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!üòéHow are you?üëª\n",
      "Welcomeüêß to the worldüåç of Python Programming.üêç\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9d190599-48a0-479e-86e1-f7414635197e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!üòéHow',\n",
       " 'are',\n",
       " 'you?üëª\\nWelcomeüêß',\n",
       " 'to',\n",
       " 'the',\n",
       " 'worldüåç',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.üêç']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the object\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "#tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa097f9-20fc-4a88-8e45-43dcb9394467",
   "metadata": {},
   "source": [
    "## 2. Tab tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7696dbd3-6078-469c-88cb-fea95a2f7d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!üòéHow are you?üëª\n",
      "Welcomeüêß to the worldüåç of Python Programming.üêç\n"
     ]
    }
   ],
   "source": [
    "f = open('mydata.txt', encoding='utf-8')\n",
    "data = f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dbf28d04-314c-4bb8-86b2-2ebe77fbb188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!üòéHow are you?üëª\\nWelcomeüêß to the worldüåç of Python Programming.üêç']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the object\n",
    "tbk = TabTokenizer()\n",
    "\n",
    "#tokenize the data\n",
    "tbk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1350785-fe15-4927-ab56-61a2aaec1fe4",
   "metadata": {},
   "source": [
    "## 3. Line Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64a98cc4-7e34-48af-a9d9-c1f121c7be56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!üòéHow are you?üëª',\n",
       " 'Welcomeüêß to the worldüåç of Python Programming.üêç']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the object\n",
    "lt = LineTokenizer()\n",
    "\n",
    "#tokenize the data\n",
    "lt.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fa5674-853d-4519-8aaf-946ad9d051e3",
   "metadata": {},
   "source": [
    "## 4. White Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c48df367-3ab2-4b1a-9e7a-c95a13310b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!üòéHow',\n",
       " 'are',\n",
       " 'you?üëª',\n",
       " 'Welcomeüêß',\n",
       " 'to',\n",
       " 'the',\n",
       " 'worldüåç',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.üêç']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the object\n",
    "wst = WhitespaceTokenizer()  #like split considers all three spaces- space, tab, newline\n",
    "\n",
    "#tokenize the data\n",
    "wst.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bafc8b-b424-49cd-ad0d-c09ed6ea7427",
   "metadata": {},
   "source": [
    "## 5. MWE Tokenizer (Multi word expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a842748-cb7f-48f5-aacc-0524d3d518db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Van Rossum is Python creator, visiting Pune this week. The development community is very eager to meet Van Rossum.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'The Van Rossum is Python creator, visiting Pune this week. The development community is very eager to meet Van Rossum.'\n",
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3ddf99a-42a0-42a2-b357-aca66e23f275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1882cf7-4ac1-4c9a-b2dd-60debf4af511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the object\n",
    "tk = MWETokenizer(separator=' ')  #when we want two words to be together as same token\n",
    "\n",
    "#add multi word expression\n",
    "tk.add_mwe(('Van','Rossum'))\n",
    "\n",
    "#tokenize the data\n",
    "tk.tokenize(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d8f38-9683-44e3-91b0-9646115186af",
   "metadata": {},
   "source": [
    "## 6. Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "430bf99e-662f-429c-90ed-72acb2273e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends :)! How are you? Welcome to the world of Python Programming. :D\n"
     ]
    }
   ],
   "source": [
    "sent = 'Hello Friends :)! How are you? Welcome to the world of Python Programming. :D'\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3883a2c2-614a-486a-b5c4-f44751ef1bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ':)',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.',\n",
       " ':D']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the object\n",
    "tk = TweetTokenizer()  \n",
    "\n",
    "#tokenize the data\n",
    "tk.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb21cf95-35dd-40de-900a-7b51d5bc4ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!üòéHow are you?üëª\n",
      "Welcomeüêß to the worldüåç of Python Programming.üêç\n"
     ]
    }
   ],
   "source": [
    "f = open('mydata.txt',encoding='utf-8')\n",
    "data = f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a056271f-88b3-4ee1-a23d-faeec9e17268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " 'üòé',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'üëª',\n",
       " 'Welcome',\n",
       " 'üêß',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'üåç',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.',\n",
       " 'üêç']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the object\n",
    "tk = TweetTokenizer()  \n",
    "\n",
    "#tokenize the data\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb15d322-6c6b-472d-9bc6-47a94d17f40a",
   "metadata": {},
   "source": [
    "## 7. Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75bdfc60-0a61-4287-aa74-b757eac9d04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "This is some text\n",
      " with punctuation\n",
      "  > Let's tokenize it\n",
      " Is it ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,:?]+\", text)\n",
    "\n",
    "text = \"This is some text? with punctuation.  > Let's tokenize it. Is it ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d9324d9-aec5-49b6-8cb9-1e248376b6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('student3.tsv')\n",
    "data = f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c90e9be-6424-4021-8dfc-4280467c4306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"\\n\", text)\n",
    "\n",
    "tokens = custom_tokenizer(data)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "lst = []\n",
    "\n",
    "for token in tokens:\n",
    "    lst.append(word_tokenize(token))\n",
    "\n",
    "lst.pop(0)\n",
    "lst.pop(-1)\n",
    "lst2 = [[int(x[0]), x[1], x[2], float(x[3]), int(x[4])] for x in lst]\n",
    "lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96e54861-2836-4c25-ad85-8f4784c5f2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = []\n",
    "\n",
    "for x in data.split('\\n'):\n",
    "    inner_list = []\n",
    "    for y in x.split('\\t'):\n",
    "        if y.isdigit():\n",
    "            inner_list.append(int(y))\n",
    "        elif y.find('.') > 0 :\n",
    "            inner_list.append(float(y))\n",
    "        else :\n",
    "            inner_list.append(y)\n",
    "    new_data.append(inner_list)\n",
    "\n",
    "new_data.pop(0)\n",
    "new_data.pop(-1)\n",
    "new_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
