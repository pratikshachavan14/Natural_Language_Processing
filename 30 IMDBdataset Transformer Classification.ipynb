{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "id": "hRp_yEbmZHbU",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XobHcVOtY68r"
   },
   "source": [
    "#### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pnhQ8q98YnYQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBdgaQg2KyrD"
   },
   "source": [
    "### Define the transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QDEpQ1NPKyrD"
   },
   "outputs": [],
   "source": [
    "class TransofmerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "       # embed_dim: This parameter specifies the dimentionality of the input and output embeddings\n",
    "       # num_heads: This parameter controls the number of attention heads in the\n",
    "       # ff_dim: This parameter specifies the dimensionlity of the feedforward network\n",
    "       # rate: This parameter specifies the dropout rate\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"),Dense(embed_dim),]\n",
    "        )\n",
    "        # self.ffn: This creates a feedforward network, often used for additional normalization\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        # self.layernorm1 and self.layernorm2: These create LayerNormalization layers\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        # self.dropout1 and self.dropout2: These create Dropout layers, randomly setting\n",
    "    def call(self, inputs, training):\n",
    "\n",
    "      attn_output = self.att(inputs, inputs)\n",
    "      # Applies multi-head attention to the inputs sequences, allowing different\n",
    "\n",
    "      attn_output = self.dropout1(attn_output, training=training)\n",
    "      # Applies dropout to the attention output:\n",
    "\n",
    "      out1 = self.layernorm1(inputs + attn_output)\n",
    "      # Adds the attention output to the original input and applies layer of Normalization\n",
    "\n",
    "      ffn_output = self.ffn(out1)\n",
    "      # Passes the normalized output through the feedforward network\n",
    "\n",
    "      ffn_output = self.dropout2(ffn_output, training=training)\n",
    "      # Applies dropout to the feedforward output.\n",
    "\n",
    "      return self.layernorm2(out1 + ffn_output)\n",
    "      # Adds the feedforward output to the previous layers output and applies final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "09FTLPn7KyrE"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "  def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "    # maxlen : The maximum length of the input sequences the model will handle.\n",
    "    # vocab_size : The total number of unique tokens (words) in the vocabulary\n",
    "    # embed_dim : The dimesionality of the embeddings\n",
    "        # (how each token and its position will be represented as vector)\n",
    "    super().__init__()\n",
    "    self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "    # An Embedding layer that maps each token in the input sequence to a dense vector of size embed_size\n",
    "    self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "    # An embedding layer that maps each position in the sequence from 0 to maxlen-1 to a dense vector of size embed_dim\n",
    "\n",
    "  def call(self, x):\n",
    "    maxlen = tf.shape(x)[-1]\n",
    "    # extracts the actual length of the current input sequences\n",
    "    positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "    # creates a tensor of positions from 0 to maxlen-1\n",
    "    positions = self.pos_emb(positions)\n",
    "    # Looks up the position embeddings for each position in the sequence\n",
    "    x = self.token_emb(x)\n",
    "    # Looks up the token embeddings for each token in the input sequence\n",
    "    return x + positions\n",
    "    # Adds the token embeddigns and position embeddings\n",
    "    # resulting in a combined representation that captures both word\n",
    "    # meaning and position information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xnwgXBdkKyrE"
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000 # Only consider the top 20k words\n",
    "maxlen = 150 # Only consider the first 150 words of each movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUP0RjxRKyrF",
    "outputId": "ae14a487-1015-4214-cd81-2bb0c2ca7e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 1s 0us/step\n",
      "25000 Training  sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), 'Training  sequences')\n",
    "print(len(x_val), 'Validation sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BEb1UUHcKyrF"
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlPM7mltKyrF",
    "outputId": "3f303731-d3d5-491a-b90d-406bc727ffec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 150)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-_uIw1bKyrF",
    "outputId": "7bb9f24d-23de-4b16-89a4-c8b2eba55dcd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2143,    48,    13,    69,     6, 12928,    13,    62,    28,\n",
       "        2564,    12,     8,    98,   634,   908,    10,    10,  2047,\n",
       "        3423,     9, 14790,    17,     2,     6,    87,  1465,    48,\n",
       "          25,   377,    27,   478,   157,    11,     2, 18497,    29,\n",
       "        2010,     4,  2915,     7,  5712, 12710,    83,     6,  3207,\n",
       "           2,     7,   107,    42,   289,   715,   257,     5,    95,\n",
       "        9727,     4, 13331,    11,    17, 10846,     5, 13869,  1377,\n",
       "          17,   614,    11,    14,   365,  1652,     2,     2,   373,\n",
       "          10,    10,     4,   167,  6184,     2,   287,    64,    35,\n",
       "           2,  3470,     7,  1489,     4,   370,   121,    12,    80,\n",
       "         123,   178,    51,    75,   181,     8,    67,     4,   636,\n",
       "       10227,     9,  3735,  3316,   190,    50,     9,   486,    54,\n",
       "          11,     6,   303,   548,  6548,   684,  8135,     2,   208,\n",
       "          11,     4,     2,     2,    95,  5115,     4,  4154,  5425,\n",
       "         190,   122,    15,    79,   143,    10,    10,  1479,  1468,\n",
       "           9,     6,   196,   297,    14,   310,     9,    24,  1178,\n",
       "          18, 10552,   361,    42,    76,   334], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wyjPJyGGYhVD"
   },
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "num_heads = 2\n",
    "ff_dim = 32\n",
    "\n",
    "inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransofmerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(20, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "outputs = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0rHc3clZ49U",
    "outputId": "469c5edd-f62a-4da3-cfcb-77b72a97d71a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 150, 32)           644800    \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transofmer_block (Transofm  (None, 150, 32)           10656     \n",
      " erBlock)                                                        \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 32)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 656158 (2.50 MB)\n",
      "Trainable params: 656158 (2.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vaHG6EhqbM3f",
    "outputId": "a762f9ca-a8b0-4500-dfab-37fd79157809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "782/782 [==============================] - 66s 75ms/step - loss: 0.3901 - accuracy: 0.8174 - val_loss: 0.3168 - val_accuracy: 0.8631\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.2131 - accuracy: 0.9172 - val_loss: 0.3551 - val_accuracy: 0.8621\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.1500 - accuracy: 0.9441 - val_loss: 0.4231 - val_accuracy: 0.8524\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.1067 - accuracy: 0.9624 - val_loss: 0.4587 - val_accuracy: 0.8452\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0835 - accuracy: 0.9708 - val_loss: 0.5816 - val_accuracy: 0.8434\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 15s 20ms/step - loss: 0.0593 - accuracy: 0.9812 - val_loss: 0.6526 - val_accuracy: 0.8370\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.0436 - accuracy: 0.9854 - val_loss: 0.6893 - val_accuracy: 0.8293\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.0360 - accuracy: 0.9891 - val_loss: 0.8420 - val_accuracy: 0.8242\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.0301 - accuracy: 0.9906 - val_loss: 0.7944 - val_accuracy: 0.8280\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0231 - accuracy: 0.9928 - val_loss: 0.9189 - val_accuracy: 0.8266\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.9896 - val_accuracy: 0.8258\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0162 - accuracy: 0.9950 - val_loss: 1.1691 - val_accuracy: 0.8256\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 1.0219 - val_accuracy: 0.8100\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.0103 - accuracy: 0.9976 - val_loss: 1.0573 - val_accuracy: 0.8096\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.0130 - accuracy: 0.9964 - val_loss: 1.1257 - val_accuracy: 0.8196\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.0118 - accuracy: 0.9967 - val_loss: 1.2433 - val_accuracy: 0.8192\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 1.3892 - val_accuracy: 0.8168\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.0124 - accuracy: 0.9962 - val_loss: 1.2691 - val_accuracy: 0.8111\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0080 - accuracy: 0.9980 - val_loss: 1.1811 - val_accuracy: 0.8185\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0076 - accuracy: 0.9980 - val_loss: 1.3668 - val_accuracy: 0.8144\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0064 - accuracy: 0.9981 - val_loss: 1.1155 - val_accuracy: 0.8170\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 1.3937 - val_accuracy: 0.8164\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 1.5260 - val_accuracy: 0.8159\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 1.5097 - val_accuracy: 0.8086\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0040 - accuracy: 0.9986 - val_loss: 1.7321 - val_accuracy: 0.8068\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.0029 - accuracy: 0.9989 - val_loss: 1.6272 - val_accuracy: 0.8125\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0063 - accuracy: 0.9981 - val_loss: 1.5563 - val_accuracy: 0.8128\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 1.7131 - val_accuracy: 0.8117\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 1.6834 - val_accuracy: 0.8102\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 1.8595 - val_accuracy: 0.8082\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 1.7520 - val_accuracy: 0.8106\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 1.3964 - val_accuracy: 0.8052\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 1.8674 - val_accuracy: 0.7940\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 1.8004 - val_accuracy: 0.8065\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 4.8726e-04 - accuracy: 0.9999 - val_loss: 2.5447 - val_accuracy: 0.8037\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 1.8858 - val_accuracy: 0.8052\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 1.9978 - val_accuracy: 0.8006\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 1.7250 - val_accuracy: 0.7997\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 1.9371 - val_accuracy: 0.8069\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 3.7317e-04 - accuracy: 0.9998 - val_loss: 2.2852 - val_accuracy: 0.8087\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 1.8679 - val_accuracy: 0.8011\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 1.6629 - val_accuracy: 0.8068\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 2.0039 - val_accuracy: 0.8014\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 2.2551 - val_accuracy: 0.8003\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 2.1309 - val_accuracy: 0.8008\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 2.0302 - val_accuracy: 0.7983\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 8.1791e-04 - accuracy: 0.9999 - val_loss: 2.2920 - val_accuracy: 0.8019\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 2.2606e-05 - accuracy: 1.0000 - val_loss: 2.5365 - val_accuracy: 0.8022\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 4.4432e-06 - accuracy: 1.0000 - val_loss: 2.6398 - val_accuracy: 0.8028\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 2.0499e-06 - accuracy: 1.0000 - val_loss: 2.7350 - val_accuracy: 0.8027\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.4866e-06 - accuracy: 1.0000 - val_loss: 2.8315 - val_accuracy: 0.8025\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 7.1168e-07 - accuracy: 1.0000 - val_loss: 2.9274 - val_accuracy: 0.8026\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 8.9468e-07 - accuracy: 1.0000 - val_loss: 3.0425 - val_accuracy: 0.8023\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 3.9470e-07 - accuracy: 1.0000 - val_loss: 3.1541 - val_accuracy: 0.8023\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 3.5095e-07 - accuracy: 1.0000 - val_loss: 3.2826 - val_accuracy: 0.8019\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.6263e-07 - accuracy: 1.0000 - val_loss: 3.3890 - val_accuracy: 0.8020\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 1.1061e-07 - accuracy: 1.0000 - val_loss: 3.4940 - val_accuracy: 0.8022\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 9.1581e-08 - accuracy: 1.0000 - val_loss: 3.6247 - val_accuracy: 0.8017\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.7227e-07 - accuracy: 1.0000 - val_loss: 3.8052 - val_accuracy: 0.8010\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 5.0156e-08 - accuracy: 1.0000 - val_loss: 3.9337 - val_accuracy: 0.8008\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.0442e-07 - accuracy: 1.0000 - val_loss: 4.1348 - val_accuracy: 0.8008\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 8.5401e-09 - accuracy: 1.0000 - val_loss: 4.1907 - val_accuracy: 0.8009\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 3.7019e-08 - accuracy: 1.0000 - val_loss: 4.3122 - val_accuracy: 0.8010\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.1387e-08 - accuracy: 1.0000 - val_loss: 4.4317 - val_accuracy: 0.8010\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 8.0871e-09 - accuracy: 1.0000 - val_loss: 4.5478 - val_accuracy: 0.8010\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 4.5967e-09 - accuracy: 1.0000 - val_loss: 4.6622 - val_accuracy: 0.8011\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.7643e-09 - accuracy: 1.0000 - val_loss: 4.7280 - val_accuracy: 0.8011\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.0395e-09 - accuracy: 1.0000 - val_loss: 4.7826 - val_accuracy: 0.8010\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 2.1839e-09 - accuracy: 1.0000 - val_loss: 4.9030 - val_accuracy: 0.8009\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 3.1852e-09 - accuracy: 1.0000 - val_loss: 5.1098 - val_accuracy: 0.8005\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 2.3126e-09 - accuracy: 1.0000 - val_loss: 5.2920 - val_accuracy: 0.8006\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.6975e-09 - accuracy: 1.0000 - val_loss: 5.5254 - val_accuracy: 0.8002\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 3.0041e-10 - accuracy: 1.0000 - val_loss: 5.5681 - val_accuracy: 0.8001\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 6.8664e-10 - accuracy: 1.0000 - val_loss: 5.6098 - val_accuracy: 0.8016\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.1921e-10 - accuracy: 1.0000 - val_loss: 5.6402 - val_accuracy: 0.8016\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 2.6226e-10 - accuracy: 1.0000 - val_loss: 5.7015 - val_accuracy: 0.8014\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.9550e-10 - accuracy: 1.0000 - val_loss: 5.7533 - val_accuracy: 0.8016\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 6.6757e-10 - accuracy: 1.0000 - val_loss: 5.8826 - val_accuracy: 0.8014\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.0967e-10 - accuracy: 1.0000 - val_loss: 5.9211 - val_accuracy: 0.8015\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 2.0647e-09 - accuracy: 1.0000 - val_loss: 6.0543 - val_accuracy: 0.8016\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 3.8147e-11 - accuracy: 1.0000 - val_loss: 6.0684 - val_accuracy: 0.8017\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 3.8147e-10 - accuracy: 1.0000 - val_loss: 6.1374 - val_accuracy: 0.8015\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 5.7220e-11 - accuracy: 1.0000 - val_loss: 6.1581 - val_accuracy: 0.8015\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 3.8147e-11 - accuracy: 1.0000 - val_loss: 6.1751 - val_accuracy: 0.8013\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 1.0824e-09 - accuracy: 1.0000 - val_loss: 6.3313 - val_accuracy: 0.8011\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.9073e-11 - accuracy: 1.0000 - val_loss: 6.3385 - val_accuracy: 0.8012\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 9.5367e-11 - accuracy: 1.0000 - val_loss: 6.3690 - val_accuracy: 0.8012\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 6.6757e-11 - accuracy: 1.0000 - val_loss: 6.3853 - val_accuracy: 0.8010\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 9.0599e-11 - accuracy: 1.0000 - val_loss: 6.4248 - val_accuracy: 0.8012\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 9.5367e-12 - accuracy: 1.0000 - val_loss: 6.4312 - val_accuracy: 0.8012\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.9073e-11 - accuracy: 1.0000 - val_loss: 6.4454 - val_accuracy: 0.8010\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 9.4890e-10 - accuracy: 1.0000 - val_loss: 6.5825 - val_accuracy: 0.8008\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 2.3842e-11 - accuracy: 1.0000 - val_loss: 6.5920 - val_accuracy: 0.8008\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 3.8147e-11 - accuracy: 1.0000 - val_loss: 6.6163 - val_accuracy: 0.8006\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 6.6181 - val_accuracy: 0.8006\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 9.5367e-12 - accuracy: 1.0000 - val_loss: 6.6223 - val_accuracy: 0.8006\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 8.5831e-11 - accuracy: 1.0000 - val_loss: 6.6437 - val_accuracy: 0.8006\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 4.7684e-12 - accuracy: 1.0000 - val_loss: 6.6465 - val_accuracy: 0.8006\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 4.7684e-12 - accuracy: 1.0000 - val_loss: 6.6493 - val_accuracy: 0.8006\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 3.3379e-11 - accuracy: 1.0000 - val_loss: 6.6634 - val_accuracy: 0.8006\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=100, validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
